{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### I. Import Libraries"
      ],
      "metadata": {
        "id": "iQsupc2xdoNH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yHZwjTgUS1ar"
      },
      "outputs": [],
      "source": [
        "# !pip install llmlingua\n",
        "# !pip install spacy\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import queue\n",
        "import threading\n",
        "import time\n",
        "from llmlingua import PromptCompressor\n",
        "from datasets import load_dataset\n",
        "import random"
      ],
      "metadata": {
        "id": "jzP9VmXOS17-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check multiprocessors\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    gpu_properties = torch.cuda.get_device_properties(device)\n",
        "\n",
        "    print(f\"Device Name: {gpu_properties.name}\")\n",
        "    print(f\"Multiprocessors (SMs): {gpu_properties.multi_processor_count}\")\n",
        "    print(f\"Threads per Multiprocessor: {gpu_properties.max_threads_per_multi_processor}\")\n",
        "else:\n",
        "    print(\"CUDA not available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv--RUgWYtMt",
        "outputId": "3cd4c82e-03e4-441b-8796-7d7d3fe6d74d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Name: Tesla T4\n",
            "Multiprocessors (SMs): 40\n",
            "Threads per Multiprocessor: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### II. Load model"
      ],
      "metadata": {
        "id": "rkhT_BeVdvLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_meetingbank=\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\"\n",
        "compressor = PromptCompressor(\n",
        "    model_name=model_meetingbank,\n",
        "    use_llmlingua2=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLbsA4zgS1-_",
        "outputId": "f252c91d-923a-4a8b-b193-6847ccad6724"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### III. Define Producer and Consumer"
      ],
      "metadata": {
        "id": "LPjzr2-qd0wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate real-time data streaming\n",
        "def simulate_text_stream(text_queue):\n",
        "\n",
        "    ds_test = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
        "\n",
        "    for idx, instance in enumerate(ds_test):\n",
        "        if idx==20: break\n",
        "        text = instance['question']+instance['answer']\n",
        "        text = text[:50] # for display\n",
        "        text_queue.put(text)\n",
        "        print(f\"Queued {idx}: {text}\")\n",
        "        time.sleep(random.random())  # Simulate delay between streaming texts\n",
        "    text_queue.put(\"STOP\")  # Signal to stop processing"
      ],
      "metadata": {
        "id": "kq-mqBnqWRZC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for real-time inference\n",
        "def process_text_stream(text_queue, results_queue):\n",
        "\n",
        "  while True:\n",
        "    # Retrieve text from the queue\n",
        "    text = text_queue.get()\n",
        "    if text == \"STOP\":\n",
        "      break\n",
        "\n",
        "    results = compressor.compress_prompt_llmlingua2(\n",
        "      text,\n",
        "      rate=0.6,\n",
        "      force_tokens=['\\n', '.', '!', '?', ','],\n",
        "      chunk_end_tokens=['.', '\\n'],\n",
        "      return_word_label=True,\n",
        "      drop_consecutive=True\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    results_queue.put((text, results['compressed_prompt']))\n",
        "    print(f\"Compressed Prompt: {results['compressed_prompt']}\")"
      ],
      "metadata": {
        "id": "BjrzUFHDS2B2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Queues for streaming data and results\n",
        "text_queue = queue.Queue()\n",
        "results_queue = queue.Queue()\n",
        "\n",
        "# Start the text stream simulation in a separate thread\n",
        "threading.Thread(target=simulate_text_stream, args=(text_queue,), daemon=True).start()\n",
        "\n",
        "# Start real-time inference\n",
        "process_text_stream(text_queue, results_queue)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4pesENQeUrq",
        "outputId": "6d8da8b7-3fc6-4fa5-b2cd-332769777973"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Queued 0: Janet’s ducks lay 16 eggs per day. She eats three \n",
            "Queued 1: A robe takes 2 bolts of blue fiber and half that m\n",
            "Queued 2: Josh decides to try flipping a house.  He buys a h\n",
            "Compressed Prompt: Janet’s ducks lay 16. eats\n",
            "Compressed Prompt: robe takes 2 bolts blue fiber half\n",
            "Queued 3: James decides to run 3 sprints 3 times a week.  He\n",
            "Compressed Prompt: Josh decides try flipping house. buys\n",
            "Compressed Prompt: James run 3 sprints times week.\n",
            "Queued 4: Every day, Wendi feeds each of her chickens three \n",
            "Compressed Prompt: , Wendi feeds chickens three\n",
            "Queued 5: Kylar went to the store to buy glasses for his new\n",
            "Compressed Prompt: Kylar went store buy glasses new\n",
            "Queued 6: Toulouse has twice as many sheep as Charleston. Ch\n",
            "Compressed Prompt: Toulouse twice sheep Charleston.\n",
            "Queued 7: Carla is downloading a 200 GB file. Normally she c\n",
            "Queued 8: John drives for 3 hours at a speed of 60 mph and t\n",
            "Compressed Prompt: Carla downloading 200 GB file.\n",
            "Compressed Prompt: John drives 3 hours speed 60 mph\n",
            "Queued 9: Eliza's rate per hour for the first 40 hours she w\n",
            "Queued 10: A new program had 60 downloads in the first month.\n",
            "Compressed Prompt: Eliza's rate hour first 40 hours\n",
            "Compressed Prompt: program 60 downloads first month.\n",
            "Queued 11: Toula went to the bakery and bought various types \n",
            "Compressed Prompt: Toula went bakery bought types\n",
            "Queued 12: Carlos is planting a lemon tree. The tree will cos\n",
            "Queued 13: Melanie is a door-to-door saleswoman. She sold a t\n",
            "Compressed Prompt: Carlos planting lemon tree. tree\n",
            "Compressed Prompt: Melanie door-to-door saleswoman. sold\n",
            "Queued 14: In a dance class of 20 students, 20% enrolled in c\n",
            "Compressed Prompt: dance class 20 students, 20% enrolled\n",
            "Queued 15: A merchant wants to make a choice of purchase betw\n",
            "Compressed Prompt: merchant wants choice purchase betw\n",
            "Queued 16: Two trains leave San Rafael at the same time. They\n",
            "Compressed Prompt: trains leave San Rafael time.\n",
            "Queued 17: Jill gets paid $20 per hour to teach and $30 to be\n",
            "Compressed Prompt: Jill paid $20 hour teach $30\n",
            "Queued 18: Claire makes a 3 egg omelet every morning for brea\n",
            "Compressed Prompt: Claire makes 3 egg omelet\n",
            "Queued 19: Marissa is hiking a 12-mile trail. She took 1 hour\n",
            "Compressed Prompt: Marissa hiking 12-mile. 1 hour\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* producer and consumer are fully decoupled. Allow full usage of computer resources for inference without idle machines."
      ],
      "metadata": {
        "id": "aejp3IkhezIx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xl7-t8oNTdko"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}