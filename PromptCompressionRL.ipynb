{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwCCHCm2ni-m"
   },
   "source": [
    "### I. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n7wYc64NBnF8"
   },
   "outputs": [],
   "source": [
    "# !pip install llmlingua\n",
    "# !pip install openai==0.28\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "# !pip install datasets\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_hmw4SYAzqH",
    "outputId": "74ed36c0-fb52-475e-bd0e-b4a32ec45c1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch.nn.functional as F\n",
    "import openai\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary tokenizer data\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize OpenAI API key\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R2FSy-ln3y0"
   },
   "source": [
    "### II. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MyffmNvHUaOs",
    "outputId": "5d3b6409-fc46-46b8-8489-8bb5a241036e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\")\n",
    "\n",
    "# Access the final classification layer\n",
    "classification_layer = model.classifier\n",
    "\n",
    "# Extract weights and bias\n",
    "weights = classification_layer.weight  # Shape: (num_labels, hidden_size)\n",
    "bias = classification_layer.bias       # Shape: (num_labels,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ScWcPxB9Z9S"
   },
   "source": [
    "### III. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "w6bLK3UpA6gp"
   },
   "outputs": [],
   "source": [
    "# GPT querying function\n",
    "def query_gpt(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Metric M: BLEU for summarization\n",
    "def compute_metric(ycomp, yorig):\n",
    "\n",
    "  # Tokenize the strings\n",
    "  reference_tokens = word_tokenize(ycomp)\n",
    "  candidate_tokens = word_tokenize(yorig)\n",
    "\n",
    "  # Compute BLEU score\n",
    "  bleu_score = sentence_bleu([reference_tokens], candidate_tokens, weights=(1.0, 0.0, 0, 0))\n",
    "  return bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opmE7NN-oB9p"
   },
   "source": [
    "### IV. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ykc5pVy8Qfzq"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "ds = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "#dataloader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "training_data = []\n",
    "for idx, instance in enumerate(ds):\n",
    "  if idx==20: break\n",
    "  training_data.append(\"Question: \"+instance['question']+instance['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9Q2DZA5oHwE"
   },
   "source": [
    "### IV. Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "SSIV5d9PA6mJ"
   },
   "outputs": [],
   "source": [
    "# parameters for optimization\n",
    "compression_rate = 0.6\n",
    "tolerance = 5\n",
    "r0 = -10.0\n",
    "epochs = 5\n",
    "lr=1e-5\n",
    "entropy_weight=0.1 #lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WjV83jQmA6om",
    "outputId": "0735830c-d756-41a7-ee0b-0958144fc6d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: -5508.287598\n",
      "Epoch 2/5, Loss: -5930.135254\n",
      "Epoch 3/5, Loss: -6402.196777\n",
      "Epoch 4/5, Loss: -6394.845215\n",
      "Epoch 5/5, Loss: -6377.056641\n"
     ]
    }
   ],
   "source": [
    "#optimizer = optim.Adam(classification_layer.parameters(), lr=lr)\n",
    "optimizer = optim.Adam([weights, bias], lr=lr)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "  total_loss = 0\n",
    "\n",
    " # for batch_idx, batch_instance in enumerate(dataloader):\n",
    "  for P in training_data:\n",
    "\n",
    "    inputs = tokenizer(P, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Forward pass with output_hidden_states=True\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    # Hidden states: a tuple with one tensor per layer\n",
    "    hidden_states = outputs.hidden_states\n",
    "    logits = torch.matmul(hidden_states[0], weights.T) + bias\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    # Decode token labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    predicted_labels = [pred for pred in predictions[0]]\n",
    "\n",
    "    # Preserved tokens\n",
    "    Pc_tokens = []\n",
    "    Pc_probs = []\n",
    "\n",
    "    for token, label, prob in zip(tokens, predicted_labels, probs[0]):\n",
    "\n",
    "     # if label==0:\n",
    "      if prob[1].item()<0.3:  #temp fix\n",
    "        continue\n",
    "\n",
    "      Pc_tokens.append(token)\n",
    "      Pc_probs.append(prob[1].item())\n",
    "\n",
    "    Pc = \" \".join(Pc_tokens)\n",
    "    Pc_probs = torch.tensor(Pc_probs, requires_grad=True)\n",
    "\n",
    "    # Query GPT outputs\n",
    "    yorig = query_gpt(P)\n",
    "    ycomp = query_gpt(Pc)\n",
    "\n",
    "    # Calculate compression constraint\n",
    "    delta = len(Pc_tokens) - compression_rate * len(P.split())\n",
    "\n",
    "    # Compute reward\n",
    "    if abs(delta) <= tolerance:\n",
    "        reward = compute_metric(ycomp, yorig)\n",
    "    else:\n",
    "        reward = r0  # Penalize constraint violation\n",
    "\n",
    "    # Compute loss L\n",
    "    entropy = -torch.sum(probs * torch.log(probs))\n",
    "    loss = -reward * torch.sum(torch.log(Pc_probs) ) - entropy_weight * entropy\n",
    "\n",
    "    # Backpropagate and update policy\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss\n",
    "\n",
    "  print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4c51p-09pXQ"
   },
   "source": [
    "### VI. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "zhBdudf79osP"
   },
   "outputs": [],
   "source": [
    "ds_test = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "\n",
    "test_data = []\n",
    "for idx, instance in enumerate(ds_test):\n",
    "  if idx==3: break\n",
    "  test_data.append(\"Question: \"+instance['question']+instance['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "LxFwQoLUOInL"
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "def get_compressed_tokens(text, compression_rate, weights, bias):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # Forward pass with output_hidden_states=True\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    # Hidden states: a tuple with one tensor per layer\n",
    "    hidden_states = outputs.hidden_states\n",
    "    logits = torch.matmul(hidden_states[0], weights.T) + bias\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    # Decode token labels\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    predicted_labels = [pred for pred in predictions[0]]\n",
    "\n",
    "    # Preserved tokens\n",
    "    Pc_tokens = []\n",
    "\n",
    "    for token, label, prob in zip(tokens, predicted_labels, probs[0]):\n",
    "      #if label==0:\n",
    "      if prob[1].item()<0.3:\n",
    "        continue\n",
    "\n",
    "      Pc_tokens.append(token)\n",
    "\n",
    "    Pc = \"\".join(Pc_tokens)\n",
    "    return Pc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "gXTzgmx9qHQr"
   },
   "outputs": [],
   "source": [
    "# test\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\")\n",
    "\n",
    "# Access the final classification layer\n",
    "classification_layer = model.classifier\n",
    "\n",
    "# Extract weights and bias\n",
    "weights_org = classification_layer.weight  # Shape: (num_labels, hidden_size)\n",
    "bias_org = classification_layer.bias       # Shape: (num_labels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q_gdAoZeb6-b",
    "outputId": "32c30a0e-b06a-4a96-a36f-a6a6ffd8333f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0129,  0.0172, -0.0052,  ...,  0.0288, -0.0090,  0.0378],\n",
       "        [-0.0337, -0.0032,  0.0047,  ..., -0.0004, -0.0192,  0.0056]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NS3Zy3FHb_Ia",
    "outputId": "b55d9766-359a-400a-d4fd-612cacdbd21c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0089,  0.0204, -0.0010,  ...,  0.0328, -0.0047,  0.0358],\n",
       "        [-0.0376, -0.0065,  0.0005,  ..., -0.0043, -0.0236,  0.0076]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k3SvbeRhpLIM",
    "outputId": "a034c194-dd10-4fe1-980a-e39bf38933fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================\n",
      "Original Promot\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n",
      "Compressed Tokens using LLMLingua2\n",
      "<s>▁Question:▁Janet’s▁ducks▁lay▁16▁eggs▁per▁day.▁She▁eats▁three▁for▁breakfast▁morning▁bakes▁muffins▁for▁her▁friends▁day▁with▁four.▁She▁sells▁the▁remainder▁at▁the▁farmers'▁market▁daily▁for▁$2▁per▁fresh▁duck▁egg.▁much▁in▁dollars▁does▁she▁day▁at▁the▁farmers'Janet▁sells▁16▁-▁3▁-▁4▁=16-3-499▁duck▁eggs▁a▁day.▁She▁makes▁9▁2▁=▁$<<9*21818▁day▁at▁the▁farmer’s.▁####▁18</s>\n",
      "Compressed Tokens after Policy Optimization\n",
      "<s>▁Question▁Janets▁ducks▁lay▁16s▁peratsess▁friendssders'▁duck▁eggsJanet▁sells▁16▁3▁4163-499▁ducks▁9▁2921818s▁18</s>\n",
      "=======================\n",
      "Original Promot\n",
      "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?It takes 2/2=<<2/2=1>>1 bolt of white fiber\n",
      "So the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric\n",
      "#### 3\n",
      "Compressed Tokens using LLMLingua2\n",
      "<s>▁Question:▁robe▁takes▁2s▁of▁blue▁fiber▁half▁that▁much▁white▁fiber.s▁in▁total▁does▁it?It▁takes▁2/2<<2/2=11▁of▁white▁fiber▁So▁the▁total▁amount▁of▁fabric▁is▁2+1<<2+1=33s▁of▁fabric###▁3</s>\n",
      "Compressed Tokens after Policy Optimization\n",
      "<s>▁Question▁robe▁2ssIt▁2/22/21▁So▁amount▁fabric▁223s▁3</s>\n",
      "=======================\n",
      "Original Promot\n",
      "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?The cost of the house and repairs came out to 80,000+50,000=$<<80000+50000=130000>>130,000\n",
      "He increased the value of the house by 80,000*1.5=<<80000*1.5=120000>>120,000\n",
      "So the new value of the house is 120,000+80,000=$<<120000+80000=200000>>200,000\n",
      "So he made a profit of 200,000-130,000=$<<200000-130000=70000>>70,000\n",
      "#### 70000\n",
      "Compressed Tokens using LLMLingua2\n",
      "<s>▁Question:▁Joshs▁to▁try▁fli▁a.▁He▁buys▁a▁for▁$80,000▁and▁then▁puts▁in▁$50,000▁in▁repairs.▁This▁increased▁the▁value▁of▁the▁by▁150%.▁How▁much▁did▁he?The▁of▁the▁house▁and▁repairs▁came▁out▁to▁80,000+50,000$<<80000+50000130000130,000▁increased▁the▁value▁of▁the▁by▁80,000*1.5<<80000*1.5120000120,000▁So▁the▁new▁value▁of▁the▁is▁120,000+80,000$<<120000+80000=200000200,000▁So▁he▁made▁a▁of▁200,000-130,000$<<200000-1300007000070,000▁####▁70000</s>\n",
      "Compressed Tokens after Policy Optimization\n",
      "<s>▁Question▁Joshss80,000s50,000s▁150%Thes▁out80,00050,00080000+50000130000130,00080,0001.5800001.5120000120,000▁120,00080,000120000800000000200000▁So▁200,000-130,000200000-1300007000070,000▁70000</s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "\n",
    "  Pc = get_compressed_tokens(test_data[i], compression_rate, weights_org, bias_org)\n",
    "  Pc_rl = get_compressed_tokens(test_data[i], compression_rate, weights, bias)\n",
    "  print(\"=======================\")\n",
    "  print(\"Original Promot\")\n",
    "  print(test_data[i])\n",
    "  print(\"Compressed Tokens using LLMLingua2\")\n",
    "  print(Pc)\n",
    "  print(\"Compressed Tokens after Policy Optimization\")\n",
    "  print(Pc_rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cp0djvSLw8yu"
   },
   "source": [
    "* This work is based on the paper https://arxiv.org/pdf/2409.13035v2\n",
    "* The original paper aims to optimize the transformer encoder from LLMLingua2 via minimizing the difference between GPT3.5-generated feedbacks using original prompt and compressed prompt, while this work aims to optimize the weights&bias in the classification layer.\n",
    "* Soft constrain on compression rate via a penalized term r0 during reward update\n",
    "* Policy Optimization tends to keep numbers and operators from original prompt.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is-N4_y6nh3b"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
